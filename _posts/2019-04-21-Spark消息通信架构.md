---
layout:     post
title:      Spark消息通信架构
subtitle:   Spark消息通信架构
date:       2019-04-21
author:     CXR
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - Spark
    - 大数据
    - 消息通信
---

## Spark消息通信架构

在Spark2.0之前Spark消息通信使用的是Akka,在2.0以后的版本中采用了Netty的具体方法，来实现Spark的消息通信架构。

在框架中以**RpcEndPoint**和**RpcEndPointRef**实现了Actor和ActorRef相关动作， **RpcEndPointRef**是**RpcEndPoint** 的引用。

通信框架使用了工厂设计模式，实现了对Nett有的解耦，能够根据需要引入其他的消息通信工具。如图所示：

![Spark消息通信类图](/pic/spark_netty.png "Spark消息通信类图")

首先定义了**RpcEnv**和**RpcEnvFactory**两个抽象类，在RpcEnv定义了RPC通信框架的启动、停止、关闭等抽象方法，在RpcEnvFactory中定义了创建抽象方法。

RpcEnv源码：
```
private[spark] abstract class RpcEnv(conf: SparkConf) {

  private[spark] val defaultLookupTimeout = RpcUtils.lookupRpcTimeout(conf)

  /**
   * 返回RpcEndpoint的引用，将用于实现[[RpcEndpoint.self]]，如果RpcEndpointRef
   * 不存在返回null
   */
  private[rpc] def endpointRef(endpoint: RpcEndpoint): RpcEndpointRef

  /**
   * 返回RpcEnv 的地址
   */
  def address: RpcAddress

  /**
   *
   * 以name命名注册一个RpcEndpoint，返回RpcEndpointRef引用，不保证线程安全
   */
  def setupEndpoint(name: String, endpoint: RpcEndpoint): RpcEndpointRef

  /**
   * 通过uri异步注册返回RpcEndpointRef
   */
  def asyncSetupEndpointRefByURI(uri: String): Future[RpcEndpointRef]

  /**
   * 通过uri回收RpcEndpointRef。 这是阻塞操作。
   */
  def setupEndpointRefByURI(uri: String): RpcEndpointRef = {
    defaultLookupTimeout.awaitResult(asyncSetupEndpointRefByURI(uri))
  }

  /**
   * 通过RpcAddress和endpointName回收RpcEndpointRef。 这是阻塞操作。
   */
  def setupEndpointRef(address: RpcAddress, endpointName: String): RpcEndpointRef = {
    setupEndpointRefByURI(RpcEndpointAddress(address, endpointName).toString)
  }

  /**
   * 停止`endpoint`指定的[[RpcEndpoint]]。
   */
  def stop(endpoint: RpcEndpointRef): Unit

  /**
   * Shutdown this [[RpcEnv]] asynchronously. If need to make sure [[RpcEnv]] exits successfully,
     * call [[awaitTermination()]] straight after [[shutdown()]].
   */
  def shutdown(): Unit

  /**
   * Wait until [[RpcEnv]] exits.
   *
   * TODO do we need a timeout parameter?
   */
  def awaitTermination(): Unit

  /**
   * [[RpcEndpointRef]] cannot be deserialized without [[RpcEnv]]. So when deserializing any object
   * that contains [[RpcEndpointRef]]s, the deserialization codes should be wrapped by this method.
   */
  def deserialize[T](deserializationAction: () => T): T

  /**
   * Return the instance of the file server used to serve files. This may be `null` if the
   * RpcEnv is not operating in server mode.
   */
  def fileServer: RpcEnvFileServer

  /**
   * Open a channel to download a file from the given URI. If the URIs returned by the
   * RpcEnvFileServer use the "spark" scheme, this method will be called by the Utils class to
   * retrieve the files.
   *
   * @param uri URI with location of the file.
   */
  def openChannel(uri: String): ReadableByteChannel
}
```

RpcEnvFactory源码：
```
/**
 * A factory class to create the [[RpcEnv]]. It must have an empty constructor so that it can be
 * created using Reflection.
 */
private[spark] trait RpcEnvFactory {

  def create(config: RpcEnvConfig): RpcEnv
}
```

在**NettyRpcEnvFactory**和**NettyRpcEnv**中使用Netty对继承的方法进行了实现

NettyRpcEnvFactory源码：
```
private[rpc] class NettyRpcEnvFactory extends RpcEnvFactory with Logging {

  def create(config: RpcEnvConfig): RpcEnv = {
    val sparkConf = config.conf
    // 在多个线程中使用JavaSerializerInstance是安全的。
    //但是，如果我们计划将来支持KryoSerializer，我们必须使用ThreadLocal来存
    //储SerializerInstance
    val javaSerializerInstance =
      new JavaSerializer(sparkConf).newInstance().asInstanceOf[JavaSerializerInstance]
    val nettyEnv =
      new NettyRpcEnv(sparkConf, javaSerializerInstance, config.advertiseAddress,
        config.securityManager, config.numUsableCores)
    if (!config.clientMode) {
      val startNettyRpcEnv: Int => (NettyRpcEnv, Int) = { actualPort =>
        nettyEnv.startServer(config.bindAddress, actualPort)
        (nettyEnv, nettyEnv.address.port)
      }
      try {
        Utils.startServiceOnPort(config.port, startNettyRpcEnv, sparkConf, config.name)._1
      } catch {
        case NonFatal(e) =>
          nettyEnv.shutdown()
          throw e
      }
    }
    nettyEnv
  }
}
```

NettyRpcEnv中启动终端点方法是setupEndpoint，部分源码如下：
```
private[netty] class NettyRpcEnv(
    val conf: SparkConf,
    javaSerializerInstance: JavaSerializerInstance,
    host: String,
    securityManager: SecurityManager,
    numUsableCores: Int) extends RpcEnv(conf) with Logging {

    private val dispatcher: Dispatcher = new Dispatcher(this, numUsableCores)
    //启动终端的方法
    override def setupEndpoint(name: String, endpoint: RpcEndpoint): RpcEndpointRef = {
    dispatcher.registerRpcEndpoint(name, endpoint)
  }

}
```

通过Dispatcher的registerRpcEndpoint方法去注册RpcEndpoint，这个方法会吧**RpcEndPoint**和**RpcEndPointRef**以键值方式存放在线程安全的ConcurrentMap中：
```
private val endpointRefs: ConcurrentMap[RpcEndpoint, RpcEndpointRef] =
    new ConcurrentHashMap[RpcEndpoint, RpcEndpointRef]

def registerRpcEndpoint(name: String, endpoint: RpcEndpoint): NettyRpcEndpointRef = {
   val addr = RpcEndpointAddress(nettyEnv.address, name)
   val endpointRef = new NettyRpcEndpointRef(nettyEnv.conf, addr, nettyEnv)
   synchronized {
     if (stopped) {
       throw new IllegalStateException("RpcEnv has been stopped")
     }
     if (endpoints.putIfAbsent(name, new EndpointData(name, endpoint, endpointRef)) != null) {
       throw new IllegalArgumentException(s"There is already an RpcEndpoint called $name")
     }
     val data = endpoints.get(name)
     //线程安全的ConcurrentMap
     endpointRefs.put(data.endpoint, data.ref)
     receivers.offer(data)  // for the OnStart message
   }
   endpointRef
 }
```
