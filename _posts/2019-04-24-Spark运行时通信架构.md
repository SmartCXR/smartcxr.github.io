---
layout:     post
title:      Spark消息通信原理
subtitle:   Spark运行时通信架构
date:       2019-04-24
author:     CXR
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - Spark
    - 大数据
---

## 运行流程概述
用户提交应用程序时，应用程序的**SparkContext**会向**Master**发送应用注册请求，并由**Master**给该应用分配**Executor**， **Executor**启动后会向SparkContext发送注册成功消息；当**SparkContext**的**RDD**触发行动（action）操作后，将创建**RDD**的DAG，通过**DAGScheduler**进行划分**Stage**并且将**Stage**封装为**TaskSet**，接着由**TaskScheduler**向注册的**Executor**发送执行消息，**Executor**接收到任务消息后启动并运行；最后当所有任务运行时，由**Driver**处理结果并回收资源。


## 运行流程详细步骤与源码

（1）执行运用程序需要启动**SparkContext**，启动过程中会实例化**SchedulerBackend**对象，在**Standalone**模式中实际上创建的是**StandaloneSchedulerBackend**对象，在该对象启动中会继承父类**DriverEndPoint**和创建**StandaloneAppClient**的client。
在**StandaloneAppClient**的**tryRegisterAllMasters**方法中创建注册线程池**registerMasterThreadPool**，在该线程池中启动注册线程并向**Master**发送**RegisterApplication**注册应用消息。

### SparkContext部分源码
SparkContext有6个构造函数
```
def this() = this(new SparkConf())


def this(master: String, appName: String, conf: SparkConf) =
    this(SparkContext.updatedConf(conf, master, appName))

def this(
      master: String,
      appName: String,
      sparkHome: String = null,
      jars: Seq[String] = Nil,
      environment: Map[String, String] = Map()) = {
    this(SparkContext.updatedConf(new SparkConf(), master, appName, sparkHome, jars, environment))
  }

private[spark] def this(master: String, appName: String) =
      this(master, appName, null, Nil, Map())

private[spark] def this(master: String, appName: String, sparkHome: String) =
          this(master, appName, sparkHome, Nil, Map())

private[spark] def this(master: String, appName: String, sparkHome: String, jars: Seq[String]) =this(master, appName, sparkHome, jars, Map())
```

实例化SchedulerBackend是通过SparkContext的createTaskScheduler方法来返回**schedulerBackend**和
**taskScheduler**两个对象

源码如下：
```
// Create and start the scheduler
val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
_schedulerBackend = sched
_taskScheduler = ts
```

### createTaskScheduler源码

```
  /**
   * 在master Url的基础上创建taskScheduler和schedulerBackend
   * 返回cheduler backend 和  task scheduler的tuple
   */
  private def createTaskScheduler(
      sc: SparkContext,
      master: String,
      deployMode: String): (SchedulerBackend, TaskScheduler) = {
    import SparkMasterRegex._

    // 当运行在local模式的时候，任务失败不重新提交
    val MAX_LOCAL_TASK_FAILURES = 1

    master match {

      //local模式
      case "local" =>
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        //创建LocalSchedulerBackend
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1)
        scheduler.initialize(backend)
        (backend, scheduler)
      //local模式 N核心
      case LOCAL_N_REGEX(threads) =>
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*] 表示机器的cpu核数 local[N] 开启N个线程
        val threadCount = if (threads == "*") localCpuCount else threads.toInt
        if (threadCount <= 0) {
          throw new SparkException(s"Asked to run locally with $threadCount threads")
        }
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)

      case LOCAL_N_FAILURES_REGEX(threads, maxFailures) =>
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*, M] 表示启用机器所有的cpu核数，进行M次的失败尝试
        // local[N, M] 使用N线程进行M次的失败尝试
        val threadCount = if (threads == "*") localCpuCount else threads.toInt
        val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)

        //Standalone模式
      case SPARK_REGEX(sparkUrl) =>
        val scheduler = new TaskSchedulerImpl(sc)
        val masterUrls = sparkUrl.split(",").map("spark://" + _)

        //Standalone
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        (backend, scheduler)

      //分布式local模式
      case LOCAL_CLUSTER_REGEX(numSlaves, coresPerSlave, memoryPerSlave) =>
        // Check to make sure memory requested <= memoryPerSlave. Otherwise Spark will just hang.
        val memoryPerSlaveInt = memoryPerSlave.toInt
        if (sc.executorMemory > memoryPerSlaveInt) {
          throw new SparkException(
            "Asked to launch cluster with %d MB RAM / worker but requested %d MB/worker".format(
              memoryPerSlaveInt, sc.executorMemory))
        }

        val scheduler = new TaskSchedulerImpl(sc)
        val localCluster = new LocalSparkCluster(
          numSlaves.toInt, coresPerSlave.toInt, memoryPerSlaveInt, sc.conf)
        val masterUrls = localCluster.start()
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => {
          localCluster.stop()
        }
        (backend, scheduler)

      case masterUrl =>
        val cm = getClusterManager(masterUrl) match {
          case Some(clusterMgr) => clusterMgr
          case None => throw new SparkException("Could not parse Master URL: '" + master + "'")
        }
        try {
          val scheduler = cm.createTaskScheduler(sc, masterUrl)
          val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler)
          cm.initialize(scheduler, backend)
          (backend, scheduler)
        } catch {
          case se: SparkException => throw se
          case NonFatal(e) =>
            throw new SparkException("External scheduler cannot be instantiated", e)
        }
    }
  }
```
以Standalone模式中实际创建的是**StandaloneSchedulerBackend**
