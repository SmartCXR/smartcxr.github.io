---
layout:     post
title:      Spark消息通信原理
subtitle:   Spark运行时通信架构
date:       2019-04-24
author:     CXR
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - Spark
    - 大数据
---

## 运行流程概述
用户提交应用程序时，应用程序的**SparkContext**会向**Master**发送应用注册请求，并由**Master**给该应用分配**Executor**， **Executor**启动后会向SparkContext发送注册成功消息；当**SparkContext**的**RDD**触发行动（action）操作后，将创建**RDD**的DAG，通过**DAGScheduler**进行划分**Stage**并且将**Stage**封装为**TaskSet**，接着由**TaskScheduler**向注册的**Executor**发送执行消息，**Executor**接收到任务消息后启动并运行；最后当所有任务运行时，由**Driver**处理结果并回收资源。


## 运行流程详细步骤与源码

（1）执行运用程序需要启动**SparkContext**，启动过程中会实例化**SchedulerBackend**对象，在**Standalone**模式中实际上创建的是**StandaloneSchedulerBackend**对象，在该对象启动中会继承父类**DriverEndPoint**和创建**StandaloneAppClient**的client。
在**StandaloneAppClient**的**tryRegisterAllMasters**方法中创建注册线程池**registerMasterThreadPool**，在该线程池中启动注册线程并向**Master**发送**RegisterApplication**注册应用消息。

### SparkContext部分源码
SparkContext有6个构造函数
```
def this() = this(new SparkConf())


def this(master: String, appName: String, conf: SparkConf) =
    this(SparkContext.updatedConf(conf, master, appName))

def this(
      master: String,
      appName: String,
      sparkHome: String = null,
      jars: Seq[String] = Nil,
      environment: Map[String, String] = Map()) = {
    this(SparkContext.updatedConf(new SparkConf(), master, appName, sparkHome, jars, environment))
  }

private[spark] def this(master: String, appName: String) =
      this(master, appName, null, Nil, Map())

private[spark] def this(master: String, appName: String, sparkHome: String) =
          this(master, appName, sparkHome, Nil, Map())

private[spark] def this(master: String, appName: String, sparkHome: String, jars: Seq[String]) =this(master, appName, sparkHome, jars, Map())
```

实例化SchedulerBackend是通过SparkContext的createTaskScheduler方法来返回**schedulerBackend**和
**taskScheduler**两个对象

源码如下：
```
// Create and start the scheduler
val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
_schedulerBackend = sched
_taskScheduler = ts
```

### createTaskScheduler源码

```
  /**
   * 在master Url的基础上创建taskScheduler和schedulerBackend
   * 返回cheduler backend 和  task scheduler的tuple
   */
  private def createTaskScheduler(
      sc: SparkContext,
      master: String,
      deployMode: String): (SchedulerBackend, TaskScheduler) = {
    import SparkMasterRegex._

    // 当运行在local模式的时候，任务失败不重新提交
    val MAX_LOCAL_TASK_FAILURES = 1

    master match {

      //local模式
      case "local" =>
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        //创建LocalSchedulerBackend
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1)
        scheduler.initialize(backend)
        (backend, scheduler)
      //local模式 N核心
      case LOCAL_N_REGEX(threads) =>
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*] 表示机器的cpu核数 local[N] 开启N个线程
        val threadCount = if (threads == "*") localCpuCount else threads.toInt
        if (threadCount <= 0) {
          throw new SparkException(s"Asked to run locally with $threadCount threads")
        }
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)

      case LOCAL_N_FAILURES_REGEX(threads, maxFailures) =>
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*, M] 表示启用机器所有的cpu核数，进行M次的失败尝试
        // local[N, M] 使用N线程进行M次的失败尝试
        val threadCount = if (threads == "*") localCpuCount else threads.toInt
        val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)

        //Standalone模式
      case SPARK_REGEX(sparkUrl) =>
        val scheduler = new TaskSchedulerImpl(sc)
        val masterUrls = sparkUrl.split(",").map("spark://" + _)

        //StandaloneSchedulerBackend
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        (backend, scheduler)

      //分布式local模式
      case LOCAL_CLUSTER_REGEX(numSlaves, coresPerSlave, memoryPerSlave) =>
        // Check to make sure memory requested <= memoryPerSlave. Otherwise Spark will just hang.
        val memoryPerSlaveInt = memoryPerSlave.toInt
        if (sc.executorMemory > memoryPerSlaveInt) {
          throw new SparkException(
            "Asked to launch cluster with %d MB RAM / worker but requested %d MB/worker".format(
              memoryPerSlaveInt, sc.executorMemory))
        }

        val scheduler = new TaskSchedulerImpl(sc)
        val localCluster = new LocalSparkCluster(
          numSlaves.toInt, coresPerSlave.toInt, memoryPerSlaveInt, sc.conf)
        val masterUrls = localCluster.start()
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => {
          localCluster.stop()
        }
        (backend, scheduler)

      case masterUrl =>
        val cm = getClusterManager(masterUrl) match {
          case Some(clusterMgr) => clusterMgr
          case None => throw new SparkException("Could not parse Master URL: '" + master + "'")
        }
        try {
          val scheduler = cm.createTaskScheduler(sc, masterUrl)
          val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler)
          cm.initialize(scheduler, backend)
          (backend, scheduler)
        } catch {
          case se: SparkException => throw se
          case NonFatal(e) =>
            throw new SparkException("External scheduler cannot be instantiated", e)
        }
    }
  }
```
Standalone模式中实际创建的是**StandaloneSchedulerBackend**，它继承的是粗粒度的**CoarseGrainedSchedulerBackend**，在**StandaloneSchedulerBackend**中会创建**Appclient**并启动，而在**CoarseGrainedSchedulerBackend**中会创建并启动**driverEndpoint**

### CoarseGrainedSchedulerBackend部分源码
```
//Driver终端
var driverEndpoint: RpcEndpointRef = null

  //启动
 override def start() {
   val properties = new ArrayBuffer[(String, String)]
   for ((key, value) <- scheduler.sc.conf.getAll) {
     if (key.startsWith("spark.")) {
       properties += ((key, value))
     }
   }

   //创建Driver通过createDriverEndpointRe方法
   driverEndpoint = createDriverEndpointRef(properties)
 }
```

### createDriverEndpointRef方法源码

```
protected def createDriverEndpointRef(
    properties: ArrayBuffer[(String, String)]): RpcEndpointRef = {
  //rpcEnv类的setupEndpoint来启动Driver
  rpcEnv.setupEndpoint(ENDPOINT_NAME, createDriverEndpoint(properties))
}
```
RpcEnv源码详看[Spark消息通信架构](https://smartcxr.github.io/2019/04/21/Spark%E6%B6%88%E6%81%AF%E9%80%9A%E4%BF%A1%E6%9E%B6%E6%9E%84/)

### StandaloneSchedulerBackend部分源码

```
override def start() {

    //启动CoarseGrainedSchedulerBackend的start()方法，该方法上上面有介绍
    super.start()

    // SPARK-21159. The scheduler backend should only try to connect to the launcher when in client
    // mode. In cluster mode, the code that submits the application to the Master needs to connect
    // to the launcher instead.
    if (sc.deployMode == "client") {
      launcherBackend.connect()
    }

    // The endpoint for executors to talk to us
    val driverUrl = RpcEndpointAddress(
      sc.conf.get("spark.driver.host"),
      sc.conf.get("spark.driver.port").toInt,
      CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString
    val args = Seq(
      "--driver-url", driverUrl,
      "--executor-id", "{{EXECUTOR_ID}}",
      "--hostname", "{{HOSTNAME}}",
      "--cores", "{{CORES}}",
      "--app-id", "{{APP_ID}}",
      "--worker-url", "{{WORKER_URL}}")
    val extraJavaOpts = sc.conf.getOption("spark.executor.extraJavaOptions")
      .map(Utils.splitCommandString).getOrElse(Seq.empty)
    val classPathEntries = sc.conf.getOption("spark.executor.extraClassPath")
      .map(_.split(java.io.File.pathSeparator).toSeq).getOrElse(Nil)
    val libraryPathEntries = sc.conf.getOption("spark.executor.extraLibraryPath")
      .map(_.split(java.io.File.pathSeparator).toSeq).getOrElse(Nil)

    // When testing, expose the parent class path to the child. This is processed by
    // compute-classpath.{cmd,sh} and makes all needed jars available to child processes
    // when the assembly is built with the "*-provided" profiles enabled.
    val testingClassPath =
      if (sys.props.contains("spark.testing")) {
        sys.props("java.class.path").split(java.io.File.pathSeparator).toSeq
      } else {
        Nil
      }

    // Start executors with a few necessary configs for registering with the scheduler
    val sparkJavaOpts = Utils.sparkJavaOpts(conf, SparkConf.isExecutorStartupConf)
    val javaOpts = sparkJavaOpts ++ extraJavaOpts
    val command = Command("org.apache.spark.executor.CoarseGrainedExecutorBackend",
      args, sc.executorEnvs, classPathEntries ++ testingClassPath, libraryPathEntries, javaOpts)
    val webUrl = sc.ui.map(_.webUrl).getOrElse("")
    val coresPerExecutor = conf.getOption("spark.executor.cores").map(_.toInt)
    // If we're using dynamic allocation, set our initial executor limit to 0 for now.
    // ExecutorAllocationManager will send the real initial limit to the Master later.
    val initialExecutorLimit =
      if (Utils.isDynamicAllocationEnabled(conf)) {
        Some(0)
      } else {
        None
      }

    //App的描述信息
    val appDesc = ApplicationDescription(sc.appName, maxCores, sc.executorMemory, command,
      webUrl, sc.eventLogDir, sc.eventLogCodec, coresPerExecutor, initialExecutorLimit)
    //中会创建并启动Client
    client = new StandaloneAppClient(sc.env.rpcEnv, masters, appDesc, this, conf)
    client.start()
    launcherBackend.setState(SparkAppHandle.State.SUBMITTED)
    waitForRegistration()
    launcherBackend.setState(SparkAppHandle.State.RUNNING)
  }
```

### StandaloneAppClient部分源码分析

**ClientEndpoint**是**StandaloneAppClient**的一个内部类，它继承了**ThreadSafeRpcEndpoint**，**ThreadSafeRpcEndpoint**继承了**RpcEndpoint**的接口，在**RpcEndpoint**接口中定义了一系列方法请看[Spark消息通信架构](https://smartcxr.github.io/2019/04/21/Spark%E6%B6%88%E6%81%AF%E9%80%9A%E4%BF%A1%E6%9E%B6%E6%9E%84/)
```
//start()方法，启动endpoint
def start() {
  // Just launch an rpcEndpoint; it will call back into the listener.
  endpoint.set(rpcEnv.setupEndpoint("AppClient", new ClientEndpoint(rpcEnv)))
}
```

#### ClientEndpoint源码分析

```
//通过onStart方法启动registerWithMaster()
override def onStart(): Unit = {
      try {
        registerWithMaster(1)
      } catch {
        case e: Exception =>
          logWarning("Failed to connect to master", e)
          markDisconnected()
          stop()
      }
}

//注册到Master
private def registerWithMaster(nthRetry: Int) {
      // 将tryRegisterAllMasters()放入到registerMasterFutures中
      //它是一个AtomicReference原子引用，底层采用的是compareAndSwapInt实现CAS，用来做并发编程
      registerMasterFutures.set(tryRegisterAllMasters())
      registrationRetryTimer.set(registrationRetryThread.schedule(new Runnable {
        override def run(): Unit = {
          if (registered.get) {
            registerMasterFutures.get.foreach(_.cancel(true))
            registerMasterThreadPool.shutdownNow()
          } else if (nthRetry >= REGISTRATION_RETRIES) {
            markDead("All masters are unresponsive! Giving up.")
          } else {
            registerMasterFutures.get.foreach(_.cancel(true))
            registerWithMaster(nthRetry + 1)
          }
        }
      }, REGISTRATION_TIMEOUT_SECONDS, TimeUnit.SECONDS))
    }

/**
*   如果是HA的话则有多个Master进行异步的注册，返回一个JFuture类型的数组
*/
private def tryRegisterAllMasters(): Array[JFuture[_]] = {
  for (masterAddress <- masterRpcAddresses) yield {
    //注册线程池
    registerMasterThreadPool.submit(new Runnable {
      override def run(): Unit = try {
        if (registered.get) {
          return
        }
        logInfo("Connecting to master " + masterAddress.toSparkURL + "...")
        val masterRef = rpcEnv.setupEndpointRef(masterAddress, Master.ENDPOINT_NAME)
        // 发送注册请求
        masterRef.send(RegisterApplication(appDescription, self))
      } catch {
        case ie: InterruptedException => // Cancelled
        case NonFatal(e) => logWarning(s"Failed to connect to master $masterAddress", e)
      }
    })
  }
}


override def receive: PartialFunction[Any, Unit] = {
  // 接受 Master发回来的registerApplication消息，把注册标识设为true
      case RegisteredApplication(appId_, masterRef) =>
        appId.set(appId_)
        registered.set(true)
        master = Some(masterRef)
        listener.connected(appId.get)

      case ApplicationRemoved(message) =>
        markDead("Master removed our application: %s".format(message))
        stop()

      case ExecutorAdded(id: Int, workerId: String, hostPort: String, cores: Int, memory: Int) =>
        val fullId = appId + "/" + id
        logInfo("Executor added: %s on %s (%s) with %d core(s)".format(fullId, workerId, hostPort,
          cores))
        listener.executorAdded(fullId, workerId, hostPort, cores, memory)

      case ExecutorUpdated(id, state, message, exitStatus, workerLost) =>
        val fullId = appId + "/" + id
        val messageText = message.map(s => " (" + s + ")").getOrElse("")
        logInfo("Executor updated: %s is now %s%s".format(fullId, state, messageText))
        if (ExecutorState.isFinished(state)) {
          listener.executorRemoved(fullId, message.getOrElse(""), exitStatus, workerLost)
        }

      case WorkerRemoved(id, host, message) =>
        logInfo("Master removed worker %s: %s".format(id, message))
        listener.workerRemoved(id, host, message)

      case MasterChanged(masterRef, masterWebUiUrl) =>
        logInfo("Master has changed, new master is at " + masterRef.address.toSparkURL)
        master = Some(masterRef)
        alreadyDisconnected = false
        masterRef.send(MasterChangeAcknowledged(appId.get))
    }
```

Master接收到消息后会在**registerApplication**方法中将app加入到等待队列**waitingApps**中
注册成功后会发送注册成功的消息给Client

### Master的receive部分源码

master的receive方法篇幅太长，这里只列出**RegisterApplication**，**RegisterWorker**的相关源码请看前一章节[Spark启动消息通信](https://smartcxr.github.io/2019/04/22/Spark%E5%90%AF%E5%8A%A8%E6%B6%88%E6%81%AF%E9%80%9A%E4%BF%A1/)

```
case RegisterApplication(description, driver) =>
      // 如果状态为Standby则不处理
      if (state == RecoveryState.STANDBY) {
        // ignore, don't send response
      } else {
        logInfo("Registering app " + description.name)
        //创建app
        val app = createApplication(description, driver)
        //注册app
        registerApplication(app)
        logInfo("Registered app " + description.name + " with ID " + app.id)
        //将app加入到persistenceEngine持久化引擎中
        persistenceEngine.addApplication(app)
        //driver发送注册成功消息
        driver.send(RegisteredApplication(app.id, self))
        //Master的schedule方法
        schedule()
      }
```

### Master的createApplication源码
```
private def createApplication(desc: ApplicationDescription, driver: RpcEndpointRef):
     ApplicationInfo = {
   val now = System.currentTimeMillis()
   val date = new Date(now)
   val appId = newApplicationId(date)
   new ApplicationInfo(now, appId, desc, date, driver, defaultCores)
 }
```

### Master的registerApplication源码

```
private def registerApplication(app: ApplicationInfo): Unit = {
  val appAddress = app.driver.address
  if (addressToApp.contains(appAddress)) {
    logInfo("Attempted to re-register application at same address: " + appAddress)
    return
  }

  applicationMetricsSystem.registerSource(app.appSource)
  //存放app信息
  apps += app
  //存放appid和appinfo的Map
  idToApp(app.id) = app
  //存放driver终端引用和appinfo的Map
  endpointToApp(app.driver) = app
  //存放app地址和appinfo的Map
  addressToApp(appAddress) = app
  //将app放入到等待队列
  waitingApps += app
}
```

### master的schedule()方法

详细分析请看[Spark启动消息通信](https://smartcxr.github.io/2019/04/22/Spark%E5%90%AF%E5%8A%A8%E6%B6%88%E6%81%AF%E9%80%9A%E4%BF%A1/)的schedule源码，这里不做重复
这里会对startExecutorsOnWorkers() 源码进行剖析
```
private def schedule(): Unit = {
    if (state != RecoveryState.ALIVE) {
      return
    }
    // Drivers take strict precedence over executors
    val shuffledAliveWorkers = Random.shuffle(workers.toSeq.filter(_.state == WorkerState.ALIVE))
    val numWorkersAlive = shuffledAliveWorkers.size
    var curPos = 0
    for (driver <- waitingDrivers.toList) { // iterate over a copy of waitingDrivers
      // We assign workers to each waiting driver in a round-robin fashion. For each driver, we
      // start from the last worker that was assigned a driver, and continue onwards until we have
      // explored all alive workers.
      var launched = false
      var numWorkersVisited = 0
      while (numWorkersVisited < numWorkersAlive && !launched) {
        val worker = shuffledAliveWorkers(curPos)
        numWorkersVisited += 1
        if (worker.memoryFree >= driver.desc.mem && worker.coresFree >= driver.desc.cores) {
          launchDriver(worker, driver)
          waitingDrivers -= driver
          launched = true
        }
        curPos = (curPos + 1) % numWorkersAlive
      }
    }
    startExecutorsOnWorkers()
  }
```

### master的startExecutorsOnWorkers源码

```
/**
 * 启动Worker上的Executor
 */
private def startExecutorsOnWorkers(): Unit = {
  // 采用简单的FIFO策略对 waitingApps 等待队列内的app进行依次的运行
  for (app <- waitingApps) {
    val coresPerExecutor = app.desc.coresPerExecutor.getOrElse(1)
    // 如果剩下的核心小于coresPerExecutor，则不会分配剩余的核心
    if (app.coresLeft >= coresPerExecutor) {
      // 过滤掉没有足够资源来启动 executor 的Worder
      val usableWorkers = workers.toArray.filter(_.state == WorkerState.ALIVE)
        .filter(worker => worker.memoryFree >= app.desc.memoryPerExecutorMB &&
          worker.coresFree >= coresPerExecutor)
        .sortBy(_.coresFree).reverse
      //分配cpu核数
      val assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps)

      // 现在我们已经决定了为每个worker分配多少个内核，让我们分配它们
      for (pos <- 0 until usableWorkers.length if assignedCores(pos) > 0) {
        //分配Worker资源给Executor方法
        allocateWorkerResourceToExecutors(
          app, assignedCores(pos), app.desc.coresPerExecutor, usableWorkers(pos))
      }
    }
  }
}
```

### Master中allocateWorkerResourceToExecutors方法源码
```
private def allocateWorkerResourceToExecutors(
      app: ApplicationInfo,
      assignedCores: Int,
      coresPerExecutor: Option[Int],
      worker: WorkerInfo): Unit = {
    val numExecutors = coresPerExecutor.map { assignedCores / _ }.getOrElse(1)
    val coresToAssign = coresPerExecutor.getOrElse(assignedCores)
    for (i <- 1 to numExecutors) {
      //分配executor来执行Application
      val exec = app.addExecutor(worker, coresToAssign)
      //启动Executor
      launchExecutor(worker, exec)
      //将application状态设置为Running
      app.state = ApplicationState.RUNNING
    }
  }
```

### Master中launchExecutor方法

```
private def launchExecutor(worker: WorkerInfo, exec: ExecutorDesc): Unit = {
  logInfo("Launching executor " + exec.fullId + " on worker " + worker.id)
  //在worker中添加executor
  worker.addExecutor(exec)
  //发送启动Executor请求
  worker.endpoint.send(LaunchExecutor(masterUrl,
    exec.application.id, exec.id, exec.application.desc, exec.cores, exec.memory))
  exec.application.driver.send(
    ExecutorAdded(exec.id, worker.id, worker.hostPort, exec.cores, exec.memory))
}
```

### Worker的receive方法接收LaunchExecutor的请求

```
case LaunchExecutor(masterUrl, appId, execId, appDesc, cores_, memory_) =>
    //如果master不是激活状态，输出日志
      if (masterUrl != activeMasterUrl) {
        logWarning("Invalid Master (" + masterUrl + ") attempted to launch executor.")
      } else {
        try {
          logInfo("Asked to launch executor %s/%d for %s".format(appId, execId, appDesc.name))

          // 创建executor的运行工作目录
          val executorDir = new File(workDir, appId + "/" + execId)
          if (!executorDir.mkdirs()) {
            throw new IOException("Failed to create directory " + executorDir)
          }

          // 为executor创建本地目录，它们通过SPARK_EXECUTOR_DIRS环境变量传递给执行程序，
          并在应用程序完成时由Worker删除。
          val appLocalDirs = appDirectories.getOrElse(appId, {
            val localRootDirs = Utils.getOrCreateLocalRootDirs(conf)
            val dirs = localRootDirs.flatMap { dir =>
              try {
                val appDir = Utils.createDirectory(dir, namePrefix = "executor")
                Utils.chmod700(appDir)
                Some(appDir.getAbsolutePath())
              } catch {
                case e: IOException =>
                  logWarning(s"${e.getMessage}. Ignoring this directory.")
                  None
              }
            }.toSeq
            if (dirs.isEmpty) {
              throw new IOException("No subfolder can be created in " +
                s"${localRootDirs.mkString(",")}.")
            }
            dirs
          })
          appDirectories(appId) = appLocalDirs
          //创建ExecutorRunner
          val manager = new ExecutorRunner(
            appId,
            execId,
            appDesc.copy(command = Worker.maybeUpdateSSLSettings(appDesc.command, conf)),
            cores_,
            memory_,
            self,
            workerId,
            host,
            webUi.boundPort,
            publicAddress,
            sparkHome,
            executorDir,
            workerUri,
            conf,
            appLocalDirs, ExecutorState.RUNNING)
          executors(appId + "/" + execId) = manager
          //开启任务执行
          manager.start()
          coresUsed += cores_
          memoryUsed += memory_
          //发送Executor状态改变消息给Master
          sendToMaster(ExecutorStateChanged(appId, execId, manager.state, None, None))
        } catch {
          case e: Exception =>
            logError(s"Failed to launch executor $appId/$execId for ${appDesc.name}.", e)
            if (executors.contains(appId + "/" + execId)) {
              executors(appId + "/" + execId).kill()
              executors -= appId + "/" + execId
            }
            sendToMaster(ExecutorStateChanged(appId, execId, ExecutorState.FAILED,
              Some(e.toString), None))
        }
```

### ExecutorRunner源码

#### start()方法

```
private[worker] def start() {
    //创建线程启动fetchAndRunExecutor()
    workerThread = new Thread("ExecutorRunner for " + fullId) {
      override def run() { fetchAndRunExecutor() }
    }
    //开启线程运行
    workerThread.start()
    // Shutdown hook that kills actors on shutdown.
    shutdownHook = ShutdownHookManager.addShutdownHook { () =>
      // It's possible that we arrive here before calling `fetchAndRunExecutor`, then `state` will
      // be `ExecutorState.RUNNING`. In this case, we should set `state` to `FAILED`.
      if (state == ExecutorState.RUNNING) {
        state = ExecutorState.FAILED
      }
      killProcess(Some("Worker shutting down")) }
  }
```

#### fetchAndRunExecutor()方法

```
/**
   * 下载并运行ApplicationDescription中描述的执行程序
   */
  private def fetchAndRunExecutor() {
    try {
      // Launch the process
      val subsOpts = appDesc.command.javaOpts.map {
        Utils.substituteAppNExecIds(_, appId, execId.toString)
      }
      val subsCommand = appDesc.command.copy(javaOpts = subsOpts)
      //通过Application的信息和环境配置创建构造器builder
      val builder = CommandUtils.buildProcessBuilder(subsCommand, new SecurityManager(conf),
        memory, sparkHome.getAbsolutePath, substituteVariables)
      val command = builder.command()
      val formattedCommand = command.asScala.mkString("\"", "\" \"", "\"")
      logInfo(s"Launch command: $formattedCommand")

      //在构造器中添加执行目录信息
      builder.directory(executorDir)
      builder.environment.put("SPARK_EXECUTOR_DIRS", appLocalDirs.mkString(File.pathSeparator))
      // In case we are running this from within the Spark Shell, avoid creating a "scala"
      // parent process for the executor command
      builder.environment.put("SPARK_LAUNCH_WITH_SCALA", "0")

      // 添加webui的logurl

      val baseUrl =
        if (conf.getBoolean("spark.ui.reverseProxy", false)) {
          s"/proxy/$workerId/logPage/?appId=$appId&executorId=$execId&logType="
        } else {
          s"http://$publicAddress:$webUiPort/logPage/?appId=$appId&executorId=$execId&logType="
        }
      builder.environment.put("SPARK_LOG_URL_STDERR", s"${baseUrl}stderr")
      builder.environment.put("SPARK_LOG_URL_STDOUT", s"${baseUrl}stdout")

      // 启动构造器 创建CoarseGrainedExecutorBackend实例
      process = builder.start()
      val header = "Spark Executor Command: %s\n%s\n\n".format(
        formattedCommand, "=" * 40)

      // Redirect its stdout and stderr to files
      val stdout = new File(executorDir, "stdout")
      stdoutAppender = FileAppender(process.getInputStream, stdout, conf)

      val stderr = new File(executorDir, "stderr")
      Files.write(header, stderr, StandardCharsets.UTF_8)
      stderrAppender = FileAppender(process.getErrorStream, stderr, conf)

      //等待运行结束，向Worker发送退出消息
      val exitCode = process.waitFor()
      state = ExecutorState.EXITED
      val message = "Command exited with code " + exitCode
      worker.send(ExecutorStateChanged(appId, execId, state, Some(message), Some(exitCode)))
    } catch {
      case interrupted: InterruptedException =>
        logInfo("Runner thread for executor " + fullId + " interrupted")
        state = ExecutorState.KILLED
        killProcess(None)
      case e: Exception =>
        logError("Error running executor", e)
        state = ExecutorState.FAILED
        killProcess(Some(e.toString))
    }
  }
```

### Master的receive源码
Master 接收到Worker发送的ExecutorStateChanged消息，改变Executor的状态
```
case ExecutorStateChanged(appId, execId, state, message, exitStatus) =>
     val execOption = idToApp.get(appId).flatMap(app => app.executors.get(execId))
     execOption match {
       case Some(exec) =>
         val appInfo = idToApp(appId)
         val oldState = exec.state
         exec.state = state

         if (state == ExecutorState.RUNNING) {
           assert(oldState == ExecutorState.LAUNCHING,
             s"executor $execId state transfer from $oldState to RUNNING is illegal")
           appInfo.resetRetryCount()
         }
        //发送给driver状态改变消息
         exec.application.driver.send(ExecutorUpdated(execId, state, message, exitStatus, false))
         //如果executor状态是结束的
         if (ExecutorState.isFinished(state)) {
           // 从worker中移除executor
           logInfo(s"Removing executor ${exec.fullId} because it is $state")
           // 如果application已经结束了，则将状态展示到UI
           if (!appInfo.isFinished) {
             appInfo.removeExecutor(exec)
           }
           //从worker中移除executor
           exec.worker.removeExecutor(exec)

           val normalExit = exitStatus == Some(0)
           // Only retry certain number of times so we don't go into an infinite loop.
           // Important note: this code path is not exercised by tests, so be very careful when
           // changing this `if` condition.
           //非正常退出
           if (!normalExit
               && appInfo.incrementRetryCount() >= MAX_EXECUTOR_RETRIES
               && MAX_EXECUTOR_RETRIES >= 0) { // < 0 disables this application-killing path
             val execs = appInfo.executors.values
             if (!execs.exists(_.state == ExecutorState.RUNNING)) {
               logError(s"Application ${appInfo.desc.name} with ID ${appInfo.id} failed " +
                 s"${appInfo.retryCount} times; removing it")
               removeApplication(appInfo, ApplicationState.FAILED)
             }
           }
         }
         schedule()
       case None =>
         logWarning(s"Got status update for unknown executor $appId/$execId")
     }
```

### CoarseGrainedExecutorBackend

CoarseGrainedExecutorBackend中的onStart方法中，会发送注册Executor消息RegisterExecutor给DriverEndpoint

```
override def onStart() {
  logInfo("Connecting to driver: " + driverUrl)
  rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap { ref =>
    // This is a very fast action so we can use "ThreadUtils.sameThread"
    driver = Some(ref)
    //注册RegisterExecutor
    ref.ask[Boolean](RegisterExecutor(executorId, self, hostname, cores, extractLogUrls))
  }(ThreadUtils.sameThread).onComplete {
    // This is a very fast action so we can use "ThreadUtils.sameThread"
    case Success(msg) =>
      // Always receive `true`. Just ignore it
    case Failure(e) =>
      exitExecutor(1, s"Cannot register with driver: $driverUrl", e, notifyDriver = false)
  }(ThreadUtils.sameThread)
}
```
### DriverEndpoint
DriverEndpoint会接收注册消息
```
override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {

      //注册RegisterExecutor
      case RegisterExecutor(executorId, executorRef, hostname, cores, logUrls) =>
      //判断是否重复
        if (executorDataMap.contains(executorId)) {
          executorRef.send(RegisterExecutorFailed("Duplicate executor ID: " + executorId))
          context.reply(true)
        } else if (scheduler.nodeBlacklist.contains(hostname)) {
          // If the cluster manager gives us an executor on a blacklisted node (because it
          // already started allocating those resources before we informed it of our blacklist,
          // or if it ignored our blacklist), then we reject that executor immediately.
          logInfo(s"Rejecting $executorId as it has been blacklisted.")
          executorRef.send(RegisterExecutorFailed(s"Executor is blacklisted: $executorId"))
          context.reply(true)
        } else {
          // If the executor's rpc env is not listening for incoming connections, `hostPort`
          // will be null, and the client connection should be used to contact the executor.
          val executorAddress = if (executorRef.address != null) {
              executorRef.address
            } else {
              context.senderAddress
            }
          logInfo(s"Registered executor $executorRef ($executorAddress) with ID $executorId")
          addressToExecutorId(executorAddress) = executorId
          totalCoreCount.addAndGet(cores)
          totalRegisteredExecutors.addAndGet(1)
          val data = new ExecutorData(executorRef, executorAddress, hostname,
            cores, cores, logUrls)
          // This must be synchronized because variables mutated
          // in this block are read when requesting executors
          CoarseGrainedSchedulerBackend.this.synchronized {
            executorDataMap.put(executorId, data)
            if (currentExecutorIdCounter < executorId.toInt) {
              currentExecutorIdCounter = executorId.toInt
            }
            if (numPendingExecutors > 0) {
              numPendingExecutors -= 1
              logDebug(s"Decremented number of pending executors ($numPendingExecutors left)")
            }
          }
          //发送RegisteredExecutor的消息，会在CoarseGrainedSchedulerBackend接收到
          executorRef.send(RegisteredExecutor)
          // Note: some tests expect the reply to come after we put the executor in the map
          context.reply(true)
          listenerBus.post(
            SparkListenerExecutorAdded(System.currentTimeMillis(), executorId, data))
          //执行makeOffers
          makeOffers()
        }

      case StopDriver =>
        context.reply(true)
        stop()

      case StopExecutors =>
        logInfo("Asking each executor to shut down")
        for ((_, executorData) <- executorDataMap) {
          executorData.executorEndpoint.send(StopExecutor)
        }
        context.reply(true)

      case RemoveWorker(workerId, host, message) =>
        removeWorker(workerId, host, message)
        context.reply(true)

      case RetrieveSparkAppConfig =>
        val reply = SparkAppConfig(
          sparkProperties,
          SparkEnv.get.securityManager.getIOEncryptionKey(),
          fetchHadoopDelegationTokens())
        context.reply(reply)
    }
```
#### makdeOffers()方法

```
private def makeOffers() {
      // Make sure no executor is killed while some task is launching on it
      val taskDescs = CoarseGrainedSchedulerBackend.this.synchronized {
        // Filter out executors under killing
        val activeExecutors = executorDataMap.filterKeys(executorIsAlive)
        val workOffers = activeExecutors.map {
          case (id, executorData) =>
            new WorkerOffer(id, executorData.executorHost, executorData.freeCores,
              Some(executorData.executorAddress.hostPort))
        }.toIndexedSeq
        scheduler.resourceOffers(workOffers)
      }
      if (!taskDescs.isEmpty) {
        //启动执行Task
        launchTasks(taskDescs)
      }
    }
```

#### launchTasks方法
```
// Launch tasks returned by a set of resource offers
    private def launchTasks(tasks: Seq[Seq[TaskDescription]]) {
      for (task <- tasks.flatten) {
        val serializedTask = TaskDescription.encode(task)
        if (serializedTask.limit() >= maxRpcMessageSize) {
          Option(scheduler.taskIdToTaskSetManager.get(task.taskId)).foreach { taskSetMgr =>
            try {
              var msg = "Serialized task %s:%d was %d bytes, which exceeds max allowed: " +
                "spark.rpc.message.maxSize (%d bytes). Consider increasing " +
                "spark.rpc.message.maxSize or using broadcast variables for large values."
              msg = msg.format(task.taskId, task.index, serializedTask.limit(), maxRpcMessageSize)
              taskSetMgr.abort(msg)
            } catch {
              case e: Exception => logError("Exception in error callback", e)
            }
          }
        }
        else {
          val executorData = executorDataMap(task.executorId)
          executorData.freeCores -= scheduler.CPUS_PER_TASK

          logDebug(s"Launching task ${task.taskId} on executor id: ${task.executorId} hostname: " +
            s"${executorData.executorHost}.")
          //给ExecutorEndpoint发送启动执行Task的消息
          executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))
        }
      }
    }
```
在这里稍微缕清一下思路首先通过**CoarseGrainedSchedulerBackend**的onstart()方法，向**DriverEndpoint**发送注册请求消息，**DriverEndpoint**接收到注册请求消息，经过一系列的操作，向**CoarseGrainedSchedulerBackend**发送了注册成功的消息（executorRef.send(RegisteredExecutor)），并且开始监听心跳（listenerBus.post(
  SparkListenerExecutorAdded(System.currentTimeMillis(), executorId, data))），然后通过makeoffers()方法来启动task任务，在**CoarseGrainedSchedulerBackend**端会接收启动task的运行任务


  ### CoarseGrainedSchedulerBackend的receive方法
  ```
  override def receive: PartialFunction[Any, Unit] = {
    //如果注册成功则创建executor对象
    case RegisteredExecutor =>
      logInfo("Successfully registered with driver")
      try {
        executor = new Executor(executorId, hostname, env, userClassPath, isLocal = false)
      } catch {
        case NonFatal(e) =>
          exitExecutor(1, "Unable to create executor due to " + e.getMessage, e)
      }

    case RegisterExecutorFailed(message) =>
      exitExecutor(1, "Slave registration failed: " + message)

    // 接收启动task消息任务
    case LaunchTask(data) =>
      if (executor == null) {
        exitExecutor(1, "Received LaunchTask command but executor was null")
      } else {
        val taskDesc = TaskDescription.decode(data.value)
        logInfo("Got assigned task " + taskDesc.taskId)
        //通过executor的的launchTask方法来启动任务
        executor.launchTask(this, taskDesc)
      }
  }
  ```

  ### Executor的launchTask方法

  ```
  def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {
  //创建一个TaskRunner对象，它继承了Runnable
  val tr = new TaskRunner(context, taskDescription)
  //放入到runningTasks，它是一个线程安全的ConcurrentHashMap
  runningTasks.put(taskDescription.taskId, tr)
  //线程池运行任务统一调度
  threadPool.execute(tr)
}
  ```
TaskRunner任务执行完毕后会向DriverEndpoint发送状态变更消息StatusUpdate

### DriverEndPoint的receive方法
```
override def receive: PartialFunction[Any, Unit] = {
      case StatusUpdate(executorId, taskId, state, data) =>
      //先调用scheduler的状态改变方法，将task任务设置为完成，并且移除相关资源
        scheduler.statusUpdate(taskId, state, data.value)
        if (TaskState.isFinished(state)) {
          executorDataMap.get(executorId) match {
            case Some(executorInfo) =>
            //回收cpu资源
              executorInfo.freeCores += scheduler.CPUS_PER_TASK
              makeOffers(executorId)
            case None =>
              // Ignoring the update since we don't know about the executor.
              logWarning(s"Ignored task status update ($taskId state $state) " +
                s"from unknown executor with ID $executorId")
          }
        }
      }
```
