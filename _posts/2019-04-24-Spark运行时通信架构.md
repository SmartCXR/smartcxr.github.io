---
layout:     post
title:      Spark消息通信原理
subtitle:   Spark运行时通信架构
date:       2019-04-24
author:     CXR
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - Spark
    - 大数据
---

## 运行流程概述
用户提交应用程序时，应用程序的**SparkContext**会向**Master**发送应用注册请求，并由**Master**给该应用分配**Executor**， **Executor**启动后会向SparkContext发送注册成功消息；当**SparkContext**的**RDD**触发行动（action）操作后，将创建**RDD**的DAG，通过**DAGScheduler**进行划分**Stage**并且将**Stage**封装为**TaskSet**，接着由**TaskScheduler**向注册的**Executor**发送执行消息，**Executor**接收到任务消息后启动并运行；最后当所有任务运行时，由**Driver**处理结果并回收资源。


## 运行流程详细步骤与源码

（1）执行运用程序需要启动**SparkContext**，启动过程中会实例化**SchedulerBackend**对象，在**Standalone**模式中实际上创建的是**StandaloneSchedulerBackend**对象，在该对象启动中会继承父类**DriverEndPoint**和创建**StandaloneAppClient**的client。
在**StandaloneAppClient**的**tryRegisterAllMasters**方法中创建注册线程池**registerMasterThreadPool**，在该线程池中启动注册线程并向**Master**发送**RegisterApplication**注册应用消息。

### SparkContext部分源码
SparkContext有6个构造函数
```
def this() = this(new SparkConf())


def this(master: String, appName: String, conf: SparkConf) =
    this(SparkContext.updatedConf(conf, master, appName))

def this(
      master: String,
      appName: String,
      sparkHome: String = null,
      jars: Seq[String] = Nil,
      environment: Map[String, String] = Map()) = {
    this(SparkContext.updatedConf(new SparkConf(), master, appName, sparkHome, jars, environment))
  }

private[spark] def this(master: String, appName: String) =
      this(master, appName, null, Nil, Map())

private[spark] def this(master: String, appName: String, sparkHome: String) =
          this(master, appName, sparkHome, Nil, Map())

private[spark] def this(master: String, appName: String, sparkHome: String, jars: Seq[String]) =this(master, appName, sparkHome, jars, Map())
```

实例化SchedulerBackend是通过SparkContext的createTaskScheduler方法来返回**schedulerBackend**和
**taskScheduler**两个对象

源码如下：
```
// Create and start the scheduler
val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
_schedulerBackend = sched
_taskScheduler = ts
```

### createTaskScheduler源码

```
  /**
   * 在master Url的基础上创建taskScheduler和schedulerBackend
   * 返回cheduler backend 和  task scheduler的tuple
   */
  private def createTaskScheduler(
      sc: SparkContext,
      master: String,
      deployMode: String): (SchedulerBackend, TaskScheduler) = {
    import SparkMasterRegex._

    // 当运行在local模式的时候，任务失败不重新提交
    val MAX_LOCAL_TASK_FAILURES = 1

    master match {

      //local模式
      case "local" =>
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        //创建LocalSchedulerBackend
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1)
        scheduler.initialize(backend)
        (backend, scheduler)
      //local模式 N核心
      case LOCAL_N_REGEX(threads) =>
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*] 表示机器的cpu核数 local[N] 开启N个线程
        val threadCount = if (threads == "*") localCpuCount else threads.toInt
        if (threadCount <= 0) {
          throw new SparkException(s"Asked to run locally with $threadCount threads")
        }
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)

      case LOCAL_N_FAILURES_REGEX(threads, maxFailures) =>
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*, M] 表示启用机器所有的cpu核数，进行M次的失败尝试
        // local[N, M] 使用N线程进行M次的失败尝试
        val threadCount = if (threads == "*") localCpuCount else threads.toInt
        val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)

        //Standalone模式
      case SPARK_REGEX(sparkUrl) =>
        val scheduler = new TaskSchedulerImpl(sc)
        val masterUrls = sparkUrl.split(",").map("spark://" + _)

        //StandaloneSchedulerBackend
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        (backend, scheduler)

      //分布式local模式
      case LOCAL_CLUSTER_REGEX(numSlaves, coresPerSlave, memoryPerSlave) =>
        // Check to make sure memory requested <= memoryPerSlave. Otherwise Spark will just hang.
        val memoryPerSlaveInt = memoryPerSlave.toInt
        if (sc.executorMemory > memoryPerSlaveInt) {
          throw new SparkException(
            "Asked to launch cluster with %d MB RAM / worker but requested %d MB/worker".format(
              memoryPerSlaveInt, sc.executorMemory))
        }

        val scheduler = new TaskSchedulerImpl(sc)
        val localCluster = new LocalSparkCluster(
          numSlaves.toInt, coresPerSlave.toInt, memoryPerSlaveInt, sc.conf)
        val masterUrls = localCluster.start()
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => {
          localCluster.stop()
        }
        (backend, scheduler)

      case masterUrl =>
        val cm = getClusterManager(masterUrl) match {
          case Some(clusterMgr) => clusterMgr
          case None => throw new SparkException("Could not parse Master URL: '" + master + "'")
        }
        try {
          val scheduler = cm.createTaskScheduler(sc, masterUrl)
          val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler)
          cm.initialize(scheduler, backend)
          (backend, scheduler)
        } catch {
          case se: SparkException => throw se
          case NonFatal(e) =>
            throw new SparkException("External scheduler cannot be instantiated", e)
        }
    }
  }
```
Standalone模式中实际创建的是**StandaloneSchedulerBackend**，它继承的是粗粒度的**CoarseGrainedSchedulerBackend**，在**StandaloneSchedulerBackend**中会创建**Appclient**并启动，而在**CoarseGrainedSchedulerBackend**中会创建并启动**driverEndpoint**

### CoarseGrainedSchedulerBackend部分源码
```
//Driver终端
var driverEndpoint: RpcEndpointRef = null

  //启动
 override def start() {
   val properties = new ArrayBuffer[(String, String)]
   for ((key, value) <- scheduler.sc.conf.getAll) {
     if (key.startsWith("spark.")) {
       properties += ((key, value))
     }
   }

   //创建Driver通过createDriverEndpointRe方法
   driverEndpoint = createDriverEndpointRef(properties)
 }
```

### createDriverEndpointRef方法源码

```
protected def createDriverEndpointRef(
    properties: ArrayBuffer[(String, String)]): RpcEndpointRef = {
  //rpcEnv类的setupEndpoint来启动Driver
  rpcEnv.setupEndpoint(ENDPOINT_NAME, createDriverEndpoint(properties))
}
```
RpcEnv源码详看[Spark消息通信架构](https://smartcxr.github.io/2019/04/21/Spark%E6%B6%88%E6%81%AF%E9%80%9A%E4%BF%A1%E6%9E%B6%E6%9E%84/)

### StandaloneSchedulerBackend部分源码

```
override def start() {

    //启动CoarseGrainedSchedulerBackend的start()方法，该方法上上面有介绍
    super.start()

    // SPARK-21159. The scheduler backend should only try to connect to the launcher when in client
    // mode. In cluster mode, the code that submits the application to the Master needs to connect
    // to the launcher instead.
    if (sc.deployMode == "client") {
      launcherBackend.connect()
    }

    // The endpoint for executors to talk to us
    val driverUrl = RpcEndpointAddress(
      sc.conf.get("spark.driver.host"),
      sc.conf.get("spark.driver.port").toInt,
      CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString
    val args = Seq(
      "--driver-url", driverUrl,
      "--executor-id", "{{EXECUTOR_ID}}",
      "--hostname", "{{HOSTNAME}}",
      "--cores", "{{CORES}}",
      "--app-id", "{{APP_ID}}",
      "--worker-url", "{{WORKER_URL}}")
    val extraJavaOpts = sc.conf.getOption("spark.executor.extraJavaOptions")
      .map(Utils.splitCommandString).getOrElse(Seq.empty)
    val classPathEntries = sc.conf.getOption("spark.executor.extraClassPath")
      .map(_.split(java.io.File.pathSeparator).toSeq).getOrElse(Nil)
    val libraryPathEntries = sc.conf.getOption("spark.executor.extraLibraryPath")
      .map(_.split(java.io.File.pathSeparator).toSeq).getOrElse(Nil)

    // When testing, expose the parent class path to the child. This is processed by
    // compute-classpath.{cmd,sh} and makes all needed jars available to child processes
    // when the assembly is built with the "*-provided" profiles enabled.
    val testingClassPath =
      if (sys.props.contains("spark.testing")) {
        sys.props("java.class.path").split(java.io.File.pathSeparator).toSeq
      } else {
        Nil
      }

    // Start executors with a few necessary configs for registering with the scheduler
    val sparkJavaOpts = Utils.sparkJavaOpts(conf, SparkConf.isExecutorStartupConf)
    val javaOpts = sparkJavaOpts ++ extraJavaOpts
    val command = Command("org.apache.spark.executor.CoarseGrainedExecutorBackend",
      args, sc.executorEnvs, classPathEntries ++ testingClassPath, libraryPathEntries, javaOpts)
    val webUrl = sc.ui.map(_.webUrl).getOrElse("")
    val coresPerExecutor = conf.getOption("spark.executor.cores").map(_.toInt)
    // If we're using dynamic allocation, set our initial executor limit to 0 for now.
    // ExecutorAllocationManager will send the real initial limit to the Master later.
    val initialExecutorLimit =
      if (Utils.isDynamicAllocationEnabled(conf)) {
        Some(0)
      } else {
        None
      }

    //App的描述信息
    val appDesc = ApplicationDescription(sc.appName, maxCores, sc.executorMemory, command,
      webUrl, sc.eventLogDir, sc.eventLogCodec, coresPerExecutor, initialExecutorLimit)
    //中会创建并启动Client
    client = new StandaloneAppClient(sc.env.rpcEnv, masters, appDesc, this, conf)
    client.start()
    launcherBackend.setState(SparkAppHandle.State.SUBMITTED)
    waitForRegistration()
    launcherBackend.setState(SparkAppHandle.State.RUNNING)
  }
```

### StandaloneAppClient部分源码分析
```
//start()方法，启动endpoint
def start() {
  // Just launch an rpcEndpoint; it will call back into the listener.
  endpoint.set(rpcEnv.setupEndpoint("AppClient", new ClientEndpoint(rpcEnv)))
}

//通过onStart方法启动registerWithMaster()
override def onStart(): Unit = {
      try {
        registerWithMaster(1)
      } catch {
        case e: Exception =>
          logWarning("Failed to connect to master", e)
          markDisconnected()
          stop()
      }
}

//注册到Master
private def registerWithMaster(nthRetry: Int) {
      // 将tryRegisterAllMasters()放入到registerMasterFutures中
      //它是一个AtomicReference原子引用，底层采用的是compareAndSwapInt实现CAS，用来做并发编程
      registerMasterFutures.set(tryRegisterAllMasters())
      registrationRetryTimer.set(registrationRetryThread.schedule(new Runnable {
        override def run(): Unit = {
          if (registered.get) {
            registerMasterFutures.get.foreach(_.cancel(true))
            registerMasterThreadPool.shutdownNow()
          } else if (nthRetry >= REGISTRATION_RETRIES) {
            markDead("All masters are unresponsive! Giving up.")
          } else {
            registerMasterFutures.get.foreach(_.cancel(true))
            registerWithMaster(nthRetry + 1)
          }
        }
      }, REGISTRATION_TIMEOUT_SECONDS, TimeUnit.SECONDS))
    }

/**
*   如果是HA的话则有多个Master进行异步的注册，返回一个JFuture类型的数组
*/
private def tryRegisterAllMasters(): Array[JFuture[_]] = {
  for (masterAddress <- masterRpcAddresses) yield {
    //注册线程池
    registerMasterThreadPool.submit(new Runnable {
      override def run(): Unit = try {
        if (registered.get) {
          return
        }
        logInfo("Connecting to master " + masterAddress.toSparkURL + "...")
        val masterRef = rpcEnv.setupEndpointRef(masterAddress, Master.ENDPOINT_NAME)
        // 发送注册请求
        masterRef.send(RegisterApplication(appDescription, self))
      } catch {
        case ie: InterruptedException => // Cancelled
        case NonFatal(e) => logWarning(s"Failed to connect to master $masterAddress", e)
      }
    })
  }
}
```

Master接收到消息后会在**registerApplication**方法中将app加入到等待队列**waitingApps**中
注册成功后会发送注册成功的消息给Client

### Master的receive部分源码

master的receive方法篇幅太长，这里只列出**RegisterApplication**，**RegisterWorker**的相关源码请看前一章节[Spark启动消息通信](https://smartcxr.github.io/2019/04/22/Spark%E5%90%AF%E5%8A%A8%E6%B6%88%E6%81%AF%E9%80%9A%E4%BF%A1/)

```
case RegisterApplication(description, driver) =>
      // 如果状态为Standby则不处理
      if (state == RecoveryState.STANDBY) {
        // ignore, don't send response
      } else {
        logInfo("Registering app " + description.name)
        //创建app
        val app = createApplication(description, driver)
        //注册app
        registerApplication(app)
        logInfo("Registered app " + description.name + " with ID " + app.id)
        //将app加入到persistenceEngine持久化引擎中
        persistenceEngine.addApplication(app)
        //driver发送注册成功消息
        driver.send(RegisteredApplication(app.id, self))
        //Master的schedule方法
        schedule()
      }
```

### Master的createApplication源码
```
private def createApplication(desc: ApplicationDescription, driver: RpcEndpointRef):
     ApplicationInfo = {
   val now = System.currentTimeMillis()
   val date = new Date(now)
   val appId = newApplicationId(date)
   new ApplicationInfo(now, appId, desc, date, driver, defaultCores)
 }
```

### Master的registerApplication源码

```
private def registerApplication(app: ApplicationInfo): Unit = {
  val appAddress = app.driver.address
  if (addressToApp.contains(appAddress)) {
    logInfo("Attempted to re-register application at same address: " + appAddress)
    return
  }

  applicationMetricsSystem.registerSource(app.appSource)
  //存放app信息
  apps += app
  //存放appid和appinfo的Map
  idToApp(app.id) = app
  //存放driver终端引用和appinfo的Map
  endpointToApp(app.driver) = app
  //存放app地址和appinfo的Map
  addressToApp(appAddress) = app
  //将app放入到等待队列
  waitingApps += app
}
```

### master的schedule()方法

详细分析请看[Spark启动消息通信](https://smartcxr.github.io/2019/04/22/Spark%E5%90%AF%E5%8A%A8%E6%B6%88%E6%81%AF%E9%80%9A%E4%BF%A1/)的schedule源码，这里不做重复
这里会对startExecutorsOnWorkers() 源码进行剖析
```
private def schedule(): Unit = {
    if (state != RecoveryState.ALIVE) {
      return
    }
    // Drivers take strict precedence over executors
    val shuffledAliveWorkers = Random.shuffle(workers.toSeq.filter(_.state == WorkerState.ALIVE))
    val numWorkersAlive = shuffledAliveWorkers.size
    var curPos = 0
    for (driver <- waitingDrivers.toList) { // iterate over a copy of waitingDrivers
      // We assign workers to each waiting driver in a round-robin fashion. For each driver, we
      // start from the last worker that was assigned a driver, and continue onwards until we have
      // explored all alive workers.
      var launched = false
      var numWorkersVisited = 0
      while (numWorkersVisited < numWorkersAlive && !launched) {
        val worker = shuffledAliveWorkers(curPos)
        numWorkersVisited += 1
        if (worker.memoryFree >= driver.desc.mem && worker.coresFree >= driver.desc.cores) {
          launchDriver(worker, driver)
          waitingDrivers -= driver
          launched = true
        }
        curPos = (curPos + 1) % numWorkersAlive
      }
    }
    startExecutorsOnWorkers()
  }
```

### master的startExecutorsOnWorkers源码

```
/**
 * 启动Worker上的Executor
 */
private def startExecutorsOnWorkers(): Unit = {
  // 采用简单的FIFO策略对 waitingApps 等待队列内的app进行依次的运行
  for (app <- waitingApps) {
    val coresPerExecutor = app.desc.coresPerExecutor.getOrElse(1)
    // 如果剩下的核心小于coresPerExecutor，则不会分配剩余的核心
    if (app.coresLeft >= coresPerExecutor) {
      // 过滤掉没有足够资源来启动 executor 的Worder
      val usableWorkers = workers.toArray.filter(_.state == WorkerState.ALIVE)
        .filter(worker => worker.memoryFree >= app.desc.memoryPerExecutorMB &&
          worker.coresFree >= coresPerExecutor)
        .sortBy(_.coresFree).reverse
      //分配cpu核数
      val assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps)

      // 现在我们已经决定了为每个worker分配多少个内核，让我们分配它们
      for (pos <- 0 until usableWorkers.length if assignedCores(pos) > 0) {
        //分配Worker资源给Executor方法
        allocateWorkerResourceToExecutors(
          app, assignedCores(pos), app.desc.coresPerExecutor, usableWorkers(pos))
      }
    }
  }
}
```
未完待续
