---
layout:     post
title:      Spark作业执行原理
subtitle:   Spark作业执行原理
date:       2019-05-01
author:     CXR
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - Spark
    - 大数据
---

## 概述
![spark的standalong模式类执行关系图](/pic/spark_standalong模式执行关系类图.png "spark的standalong模式类执行关系图")

Spark的作业调度主要是基于RDD的一系列操作构成一个job，然后在Executor中执行，这些操作算子分为**转换操作（transform）**和**行动操作（action)**，只有触发了action操作才会触发job的提交。

## 提交job
如上图所示，真正的提交是从“count”这个action操作开始的，在RDD源码中count方法触发了SparkContext的runJob方法来提交作业。

SparkContext在一开始就会初始化_schedulerBackend、_taskScheduler、_dagScheduler这三个实例
```
// Create and start the scheduler
    val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
    _schedulerBackend = sched
    _taskScheduler = ts
    _dagScheduler = new DAGScheduler(this)
```

通过Spark的runJob来提交任务

### spark的runJob源码

```
def runJob[T, U: ClassTag](
     rdd: RDD[T],
     func: (TaskContext, Iterator[T]) => U,
     partitions: Seq[Int],
     resultHandler: (Int, U) => Unit): Unit = {
   if (stopped.get()) {
     throw new IllegalStateException("SparkContext has been shutdown")
   }
   val callSite = getCallSite
   val cleanedFunc = clean(func)
   logInfo("Starting job: " + callSite.shortForm)
   if (conf.getBoolean("spark.logLineage", false)) {
     logInfo("RDD's recursive dependencies:\n" + rdd.toDebugString)
   }
   //通过dagScheduler的runJob方法来提交作业
   dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
   progressBar.foreach(_.finishAll())
   rdd.doCheckpoint()
 }
```

### DAGScheduler的runJob方法

```
def runJob[T, U](
    rdd: RDD[T],
    func: (TaskContext, Iterator[T]) => U,
    partitions: Seq[Int],
    callSite: CallSite,
    resultHandler: (Int, U) => Unit,
    properties: Properties): Unit = {
  val start = System.nanoTime
  //通过submit方法来提交作业，返回成功或者失败
  val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
  ThreadUtils.awaitReady(waiter.completionFuture, Duration.Inf)
  waiter.completionFuture.value.get match {
    case scala.util.Success(_) =>
      logInfo("Job %d finished: %s, took %f s".format
        (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
    case scala.util.Failure(exception) =>
      logInfo("Job %d failed: %s, took %f s".format
        (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
      // SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.
      val callerStackTrace = Thread.currentThread().getStackTrace.tail
      exception.setStackTrace(exception.getStackTrace ++ callerStackTrace)
      throw exception
  }
}
```

### DAGScheduler的submitJob方法
```
def submitJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) => Unit,
      properties: Properties): JobWaiter[U] = {
    // Check to make sure we are not launching a task on a partition that does not exist.
    val maxPartitions = rdd.partitions.length
    partitions.find(p => p >= maxPartitions || p < 0).foreach { p =>
      throw new IllegalArgumentException(
        "Attempting to access a non-existent partition: " + p + ". " +
          "Total number of partitions: " + maxPartitions)
    }

    val jobId = nextJobId.getAndIncrement()
    if (partitions.size == 0) {
      // Return immediately if the job is running 0 tasks
      return new JobWaiter[U](this, jobId, 0, resultHandler)
    }

    assert(partitions.size > 0)
    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) => _]
    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)
    //通过DAGSchedulerEventProcessLoop的post方法进行处理，它是DAGSchduler的内部类
    eventProcessLoop.post(JobSubmitted(
      jobId, rdd, func2, partitions.toArray, callSite, waiter,
      SerializationUtils.clone(properties)))
    waiter
  }
```
DAGSchedulerEventProcessLoop继承了抽象类EventLoop，调用了它的post方法，将job放入了一个队列中

```
/**
 * 将job放入到一个队列中，之后会有线程来运行它
 */
def post(event: E): Unit = {
  eventQueue.put(event)
}
```

## 划分调度阶段
具体的stage任务划分阶段是由**DAGScheduler**实现的，**DAGScheduler**会从最后一个RDD出发，使用广度优先遍历整个依赖树，依据**宽依赖**来进行划分。在EventLoop中会通过一个线程来执行**onReceive()**方法
因为**DAGSchedulerEventProcessLoop**继承了抽象类**EventLoop**，所以会在**DAGSchedulerEventProcessLoop**对**onReceive()**方法进行具体的实现

```
override def onReceive(event: DAGSchedulerEvent): Unit = {
    val timerContext = timer.time()
    try {
      //调用doOnReceive方法
      doOnReceive(event)
    } finally {
      timerContext.stop()
    }
  }

  //doOnReceive方法
  private def doOnReceive方法(event: DAGSchedulerEvent): Unit = event match {
    //匹配到JobSubmitted
    case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>
    //调用handleJobSubmitted
      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)

    case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =>
      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)

    case StageCancelled(stageId, reason) =>
      dagScheduler.handleStageCancellation(stageId, reason)

    case JobCancelled(jobId, reason) =>
      dagScheduler.handleJobCancellation(jobId, reason)

    case JobGroupCancelled(groupId) =>
      dagScheduler.handleJobGroupCancelled(groupId)

    case AllJobsCancelled =>
      dagScheduler.doCancelAllJobs()

    case ExecutorAdded(execId, host) =>
      dagScheduler.handleExecutorAdded(execId, host)

    case ExecutorLost(execId, reason) =>
      val workerLost = reason match {
        case SlaveLost(_, true) => true
        case _ => false
      }
      dagScheduler.handleExecutorLost(execId, workerLost)

    case WorkerRemoved(workerId, host, message) =>
      dagScheduler.handleWorkerRemoved(workerId, host, message)

    case BeginEvent(task, taskInfo) =>
      dagScheduler.handleBeginEvent(task, taskInfo)

    case SpeculativeTaskSubmitted(task) =>
      dagScheduler.handleSpeculativeTaskSubmitted(task)

    case GettingResultEvent(taskInfo) =>
      dagScheduler.handleGetTaskResult(taskInfo)

    case completion: CompletionEvent =>
      dagScheduler.handleTaskCompletion(completion)

    case TaskSetFailed(taskSet, reason, exception) =>
      dagScheduler.handleTaskSetFailed(taskSet, reason, exception)

    case ResubmitFailedStages =>
      dagScheduler.resubmitFailedStages()
  }
```
由源码所示，主要的代码实现是**DAGScheduler**的**handleJobSubmitted**法

### handleJobSubmitted
```
private[scheduler] def handleJobSubmitted(jobId: Int,
     finalRDD: RDD[_],
     func: (TaskContext, Iterator[_]) => _,
     partitions: Array[Int],
     callSite: CallSite,
     listener: JobListener,
     properties: Properties) {
   var finalStage: ResultStage = null
   try {
     // 根据最后一个RDD回溯，创建finalStage
     finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)
   } catch {
     case e: BarrierJobSlotsNumberCheckFailed =>
       logWarning(s"The job $jobId requires to run a barrier stage that requires more slots " +
         "than the total number of slots in the cluster currently.")
       // If jobId doesn't exist in the map, Scala coverts its value null to 0: Int automatically.
       val numCheckFailures = barrierJobIdToNumTasksCheckFailures.compute(jobId,
         new BiFunction[Int, Int, Int] {
           override def apply(key: Int, value: Int): Int = value + 1
         })
       if (numCheckFailures <= maxFailureNumTasksCheck) {
         messageScheduler.schedule(
           new Runnable {
             override def run(): Unit = eventProcessLoop.post(JobSubmitted(jobId, finalRDD, func,
               partitions, callSite, listener, properties))
           },
           timeIntervalNumTasksCheck,
           TimeUnit.SECONDS
         )
         return
       } else {
         // Job failed, clear internal data.
         barrierJobIdToNumTasksCheckFailures.remove(jobId)
         listener.jobFailed(e)
         return
       }

     case e: Exception =>
       logWarning("Creating new stage failed due to exception - job: " + jobId, e)
       listener.jobFailed(e)
       return
   }
   // job已提交，清理内部数据
   barrierJobIdToNumTasksCheckFailures.remove(jobId)

   //根据最后一个调度阶段finalStage创建job
   val job = new ActiveJob(jobId, finalStage, callSite, listener, properties)
   clearCacheLocs()
   logInfo("Got job %s (%s) with %d output partitions".format(
     job.jobId, callSite.shortForm, partitions.length))
   logInfo("Final stage: " + finalStage + " (" + finalStage.name + ")")
   logInfo("Parents of final stage: " + finalStage.parents)
   logInfo("Missing parents: " + getMissingParentStages(finalStage))

   val jobSubmissionTime = clock.getTimeMillis()
   jobIdToActiveJob(jobId) = job
   activeJobs += job
   finalStage.setActiveJob(job)
   val stageIds = jobIdToStageIds(jobId).toArray
   val stageInfos = stageIds.flatMap(id => stageIdToStage.get(id).map(_.latestInfo))
   listenerBus.post(
     SparkListenerJobStart(job.jobId, jobSubmissionTime, stageInfos, properties))

     //提交stage，从最后一个stage开始提交
   submitStage(finalStage)
 }


 /** 提交stage，从没有父stage的开始提交 */
  private def submitStage(stage: Stage) {
    val jobId = activeJobForStage(stage)
    if (jobId.isDefined) {
      logDebug("submitStage(" + stage + ")")
      if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {
        //获取没有父stage的stage
        val missing = getMissingParentStages(stage).sortBy(_.id)
        logDebug("missing: " + missing)
        //如果没有父stage
        if (missing.isEmpty) {
          logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
          //则调用submitMissingTasks方法，提交等待的子stage，也就是task任务
          submitMissingTasks(stage, jobId.get)
        } else {
          for (parent <- missing) {
            //递归调用
            submitStage(parent)
          }
          waitingStages += stage
        }
      }
    } else {
      abortStage(stage, "No active job for stage " + stage.id, None)
    }
  }


  private def getMissingParentStages(stage: Stage): List[Stage] = {
    val missing = new HashSet[Stage]
    val visited = new HashSet[RDD[_]]
    // 存放等待访问的窄依赖RDD
    val waitingForVisit = new ArrayStack[RDD[_]]
    def visit(rdd: RDD[_]) {
      // 对访问过的RDD进行标记
      if (!visited(rdd)) {
        visited += rdd
        val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil)
        if (rddHasUncachedPartitions) {
          for (dep <- rdd.dependencies) {
            dep match {
              case shufDep: ShuffleDependency[_, _, _] =>
              //通过宽依赖进行stage的划分
                val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)
                //如果stage状态是未激活的，则加入missing中
                if (!mapStage.isAvailable) {
                  missing += mapStage
                }
              case narrowDep: NarrowDependency[_] =>
              //如果是窄依赖这，放入等待队列
                waitingForVisit.push(narrowDep.rdd)
            }
          }
        }
      }
    }
    // 以最后一个RDD开始向前遍历依赖树
    waitingForVisit.push(stage.rdd)
    while (waitingForVisit.nonEmpty) {
      //递归调用
      visit(waitingForVisit.pop())
    }
    //返回父Stage
    missing.toList
  }
    // 源码过长不全部列出
    private def submitMissingTasks(stage: Stage, jobId: Int) {
      //提交等待的子stage
      submitWaitingChildStages(stage)
    }


    //
    private def submitWaitingChildStages(parent: Stage) {
    logTrace(s"Checking if any dependencies of $parent are now runnable")
    logTrace("running: " + runningStages)
    logTrace("waiting: " + waitingStages)
    logTrace("failed: " + failedStages)

    val childStages = waitingStages.filter(_.parents.contains(parent)).toArray
    //从等待队列中移除
    waitingStages --= childStages
    for (stage <- childStages.sortBy(_.firstJobId)) {
      //提交stage运行任务
      submitStage(stage)
    }
  }
```
注：一个job中包含有一个或多个stage，stage通过宽依赖进行拆分为多个stage，至少会有一个stage

当调度阶段提交运行后，在**DAGScheduler**的**submitStage**中通过**submitMissingTasks**方法来提交一个个的task任务。

### DAGScheduler的submitMissingTasks源码
```
private def submitMissingTasks(stage: Stage, jobId: Int) {
    logDebug("submitMissingTasks(" + stage + ")")

    // First figure out the indexes of partition ids to compute.
    val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()

    // Use the scheduling pool, job group, description, etc. from an ActiveJob associated
    // with this Stage
    val properties = jobIdToActiveJob(jobId).properties

    runningStages += stage

    // 根据不同的stage生成不同的task任务
    stage match {
      case s: ShuffleMapStage =>
        outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - 1)
      case s: ResultStage =>
        outputCommitCoordinator.stageStart(
          stage = s.id, maxPartitionId = s.rdd.partitions.length - 1)
    }
    val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try {
      stage match {
        case s: ShuffleMapStage =>
          partitionsToCompute.map { id => (id, getPreferredLocs(stage.rdd, id))}.toMap
        case s: ResultStage =>
          partitionsToCompute.map { id =>
            val p = s.partitions(id)
            (id, getPreferredLocs(stage.rdd, p))
          }.toMap
      }
    } catch {
      case NonFatal(e) =>
        stage.makeNewStageAttempt(partitionsToCompute.size)
        listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))
        abortStage(stage, s"Task creation failed: $e\n${Utils.exceptionString(e)}", Some(e))
        runningStages -= stage
        return
    }

    stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)

    // If there are tasks to execute, record the submission time of the stage. Otherwise,
    // post the even without the submission time, which indicates that this stage was
    // skipped.
    if (partitionsToCompute.nonEmpty) {
      stage.latestInfo.submissionTime = Some(clock.getTimeMillis())
    }
    listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))

    // TODO: Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times.
    // Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast
    // the serialized copy of the RDD and for each task we will deserialize it, which means each
    // task gets a different copy of the RDD. This provides stronger isolation between tasks that
    // might modify state of objects referenced in their closures. This is necessary in Hadoop
    // where the JobConf/Configuration object is not thread-safe.
    var taskBinary: Broadcast[Array[Byte]] = null
    var partitions: Array[Partition] = null
    try {
      // For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).
      // For ResultTask, serialize and broadcast (rdd, func).
      var taskBinaryBytes: Array[Byte] = null
      // taskBinaryBytes and partitions are both effected by the checkpoint status. We need
      // this synchronization in case another concurrent job is checkpointing this RDD, so we get a
      // consistent view of both variables.
      RDDCheckpointData.synchronized {
        taskBinaryBytes = stage match {
          case stage: ShuffleMapStage =>
            JavaUtils.bufferToArray(
              closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef))
          case stage: ResultStage =>
            JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef))
        }

        partitions = stage.rdd.partitions
      }

      taskBinary = sc.broadcast(taskBinaryBytes)
    } catch {
      // In the case of a failure during serialization, abort the stage.
      case e: NotSerializableException =>
        abortStage(stage, "Task not serializable: " + e.toString, Some(e))
        runningStages -= stage

        // Abort execution
        return
      case NonFatal(e) =>
        abortStage(stage, s"Task serialization failed: $e\n${Utils.exceptionString(e)}", Some(e))
        runningStages -= stage
        return
    }

    val tasks: Seq[Task[_]] = try {
      val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()
      stage match {
        case stage: ShuffleMapStage =>
          stage.pendingPartitions.clear()
          partitionsToCompute.map { id =>
            val locs = taskIdToLocations(id)
            val part = partitions(id)
            stage.pendingPartitions += id
            new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),
              Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())
          }

        case stage: ResultStage =>
          partitionsToCompute.map { id =>
            val p: Int = stage.partitions(id)
            val part = partitions(p)
            val locs = taskIdToLocations(id)
            new ResultTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, id, properties, serializedTaskMetrics,
              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,
              stage.rdd.isBarrier())
          }
      }
    } catch {
      case NonFatal(e) =>
        abortStage(stage, s"Task creation failed: $e\n${Utils.exceptionString(e)}", Some(e))
        runningStages -= stage
        return
    }

    // task任务大于0
    if (tasks.size > 0) {
      logInfo(s"Submitting ${tasks.size} missing tasks from $stage (${stage.rdd}) (first 15 " +
        s"tasks are for partitions ${tasks.take(15).map(_.partitionId)})")
        //提交task任务，task任务会封装成TaskSet进行提交
      taskScheduler.submitTasks(new TaskSet(
        tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties))
    } else {
      // Because we posted SparkListenerStageSubmitted earlier, we should mark
      // the stage as completed here in case there are no tasks to run
      markStageAsFinished(stage, None)

      stage match {
        case stage: ShuffleMapStage =>
          logDebug(s"Stage ${stage} is actually done; " +
              s"(available: ${stage.isAvailable}," +
              s"available outputs: ${stage.numAvailableOutputs}," +
              s"partitions: ${stage.numPartitions})")
          //标记调度阶段已经完成
          markMapStageJobsAsFinished(stage)
        case stage : ResultStage =>
          logDebug(s"Stage ${stage} is actually done; (partitions: ${stage.numPartitions})")
      }
      submitWaitingChildStages(stage)
    }
  }
```

### TaskSchedulerImpl的submitTasks方法
在**submitTasks**中会先通过**createTaskSetManager**创建一个TaskSetManager用于管理任务集的生命周期

```
override def submitTasks(taskSet: TaskSet) {
    val tasks = taskSet.tasks
    logInfo("Adding task set " + taskSet.id + " with " + tasks.length + " tasks")
    this.synchronized {
      val manager = createTaskSetManager(taskSet, maxTaskFailures)
      val stage = taskSet.stageId
      val stageTaskSets =
        taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])
      stageTaskSets(taskSet.stageAttemptId) = manager
      val conflictingTaskSet = stageTaskSets.exists { case (_, ts) =>
        ts.taskSet != taskSet && !ts.isZombie
      }
      if (conflictingTaskSet) {
        throw new IllegalStateException(s"more than one active taskSet for stage $stage:" +
          s" ${stageTaskSets.toSeq.map{_._2.taskSet.id}.mkString(",")}")
      }
      //将任务集加入到调度池中，由系统统一调度
      //支持FIFO和FAIR两种
      //SchedulableBuilder是一个接口，他的实现类有FairSchedulableBuilder和FIFOSchedulableBuilder
      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)

      if (!isLocal && !hasReceivedTask) {
        starvationTimer.scheduleAtFixedRate(new TimerTask() {
          override def run() {
            if (!hasLaunchedTask) {
              logWarning("Initial job has not accepted any resources; " +
                "check your cluster UI to ensure that workers are registered " +
                "and have sufficient resources")
            } else {
              this.cancel()
            }
          }
        }, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS)
      }
      hasReceivedTask = true
    }
    //后台进程调用reviveOffers方法分配资源并且运行
    backend.reviveOffers()
  }
```
未完待续P114
